{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basics\n",
    "* Basically markov-chain help us to predict what will be happen tomorrow by knowing the **state** today.\n",
    "* From the **_current state_** we have an arrow pointing to the **_future state_** with the probability of prediction it (e.g. 60% chance of Pizza being served tomorrow).\n",
    "* We may also have a **_self-pointing arrow_** that shows what is the probabilty of same state happens in the future.\n",
    "* The sum of weighted arrows should be always equal to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Properties of Markov-Chains\n",
    "1. The future state depends only on the current states (no dependency after the first lag) $P(X_n+1 = x | X_n = x_n)$ (most important one)\n",
    "2. The sum of weighted arrows should be equal to 1 (there are few exceptions).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Random Walk\n",
    "* What we want to know is that if we were in a current state (hamburger) what would be the food in the next 10 future states and what would be the probability of each of them happening.\n",
    "    + the probability of hamburger is (4/10), pizza (2/10), and hotdog (4/10).\n",
    "    + what we are interested in is that does these probabilities reach to steady states or changes forever. According to Markov-Chain if our random walks goes to infinity (large enough) we'll to the probabilty of each state that is known as staionary distribution/the equilibrium state. It means that after reaching to this state our probability is not a function of time anymore. \n",
    "* However, doing it manually each time doesn't make sense and/or we can't be sure wether there are other stationary state or not. This is where linear algebra comes in handy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Linear Algebra for Markov-Chains\n",
    "* We can represent the markov-chain more efficiently by an **_Transition (adjacency) matrix_**. The elements in the matrix just denote the weight of the edge connecting the two corresponding vertices which is essentialy a directed graph.\n",
    "    + If an element is 0 then it just means that there's no edge between the two vertices\n",
    "* Our goal is to find the probabilities of each state. Conventionally we take a row vector called '$\\pi$' whose elements represent the probabilities of the states.    \n",
    "    + Suppose that we are in a pizza day so the second element which corresponds to the pizza becomes one $\\pi_0=[0,1,0]$ \n",
    "    + In the next step we should multiply our vector by our Transition matrix. What we get is the future probabilities corresponding to the pizza states. \n",
    "    + Then we get the new matrix $\\pi_0 A$, and multiply it to the transition matrix once again. We repeat this loop until we reach to the stationary state (where our input vector is exactly same with the last output vector) -> $\\pi A = \\pi$  \n",
    "    + The above equation is same with a very famous equation in linear algebra known as _eigenvector equation_ $A \\nu = \\lambda \\nu$. You can imagine that $\\lambda$ being equal to 1. another condition is that the sum of elements in our $\\pi$ vector should add up to one \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Recurrence, Irreducibility, Classes\n",
    "* If there would be no arrow from one state going to another one and _come back_ to its own as soon as we live that states we will never come back to it. \n",
    "    + This mean the sum of the probailities of that state is below 1 and we call it '**_Transient State_**' or '**_absorbing state_**'. In this case we say that the Markov-chains is **_reducable_** as we can divide or reduce this chain into smaller chains which are irriducable.\n",
    "        - If we have a reducable Markov-Chains then we can divided it to the different **_Classes_** which each class is _Irreducable_. Between any of these classes we can always go from any state to other states. That's why they are also known as _communicating classes_.\n",
    "    + but if we can go back to the current state the sum of probability is 1 and we call it '**_Recurrent State_**'\n",
    "    + If we could go back from any future states to the current states we say Markov-chains is **_Irreducable_**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. n-step (Higher Order) Transition Matrix\n",
    "* In **_higher order_** transition matrix we are interested in finding the probability of reaching state _j_ from state _i_ after exactly _n_ steps -> $ P_ij(n) $ -> $P_02(1)$.\n",
    "*  -> $P_ij(1) = A_ij$ -> $P_ij(2) = A^2_ij$ -> $P_ij(n) = A^n_ij$\n",
    "* **Chapman-Kolmogrov Theorem**: This Theorem allows us to calculate the probability of going from _i_ to _j_ in _n_ steps with an intermediate stop at state _k_ after _r_ steps.\n",
    "    - $P_ij(n) = \\sum_k P_ik(r) * P_kj(n-r)$\n",
    "* The Higher Order Transition Matrices are connected with the staitionary or equilibrium distribution (reaching to the point in the long run that probabilities doesn't change with the time). \n",
    "    + The long-run here means that after an infinite number of transition.\n",
    "    + The Elements of $A^\\infty$ represent the probability of being at _j_ after infinite steps starting from _i_. And this value is same for a fixed _j_, in other words it doesn't depend on the starting state, because the staionary distribution is a property of the whole Markov-Chains and it doesn't depend on the starting state.\n",
    "    + The rows of $A^\\infty$ will converge only when certain conditions are met. One of the condition are **Irreducibility** and **Periodicity** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hidden Markov Model\n",
    "* The hidden Markov Model shows the probability of another event given the current state. For instance the person's mood given the weather today is an example of _hidden Markov model_. If we assume that we don't have access to the town weather but we can somehow find the person's mode in a given day then the person's mood is our observed variable (emission matrix) where as the weather is our hidden states (transition matrix).\n",
    "* $HMM = Hidden MC + Observed Variables$\n",
    "* For finding the probabilty of a sunny day which is here our Hidden MC we should find the probability of a sunny day in a stationary state\n",
    "    + However because we don't have the probabilities of our Hidden MC, what we can do is to calculate all the possible combination of weather (using the Bays Theorem) responding to the mood and choose the maximum probability. \n",
    "    + The number of all possible outcomes could be calculated by $number of multiplications = 2TN^T$ which N represent the number of hidden states and T represent the length of observed sequence. As you can see the number of multiplication increases exponentialy by increasing the number of hidden states and that's the reason why we need an algorith to do all the calculations.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Forward Algorithm in Hidden Markov Model\n",
    "* It is a very elegant algorithm technique to efficiently carried out the long computation in a HMM \n",
    "* What we want to do with the algorithm is to find the probability of this observed sequence (e.g. two sad moods in a row follows by a happy mood) given the parameters of our HMM which are \n",
    "    - Transition Matrix\n",
    "    - Emission Matrix\n",
    "    - Stationary Vector\n",
    "* When we looks at all the possible multiplications for a given sequence we will witness lots of repetition. The algorithm do this multiplication and store their value and uses them whenever they repeated in somewhere else. For doing that we need to use the dynamic programming by subsetting our initial question\n",
    "    + $\\alpha_t(X_i) = \\alpha_t-1(X_0)P(X_i|X_0)P(Y^t|X_i) + \\alpha_t-1(X_1)P(X_i|X_1)P(Y_t|X_i)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps for building a Markov-Chain Simulation model\n",
    "+ Step 1: Clean the data\n",
    "    * Finding the number of customers together with their movement from one state to another using timestamp as an indicator of movement\n",
    "    * making a list of all paths taken by consumers\n",
    "    * As we said the transition matrix is essentialy a directed graph and to implement graph the best option is a nested dictionary because we not only need to store the transition from one state to other one, we also need to store the probability corresponding to that transition.\n",
    "    * The outer level stores the states as _keys_ but in the inner level we gonna stroe the transition probabilities   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "* Theoritical parts: Normalized Nerd -> Youtube Channel (Markov Chains Clearly Explained!)\n",
    "    + https://www.youtube.com/watch?v=G7FIQ9fXl6U&list=PLM8wYQRetTxBkdvBtz-gw8b9lcVkdXQKV&index=7&ab_channel=NormalizedNerdNormalizedNerd\n",
    "* Python Code: Morten Hegewald ->  Marketing Channel Attribution with Markov Chains in Python\n",
    "    + Part 1: https://medium.com/@mortenhegewald/marketing-channel-attribution-using-markov-chains-101-in-python-78fb181ebf1e\n",
    "    + Part 2: https://towardsdatascience.com/marketing-channel-attribution-with-markov-chains-in-python-part-2-the-complete-walkthrough-733c65b23323"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
